{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing_Approach_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmhvlVpi4TNK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a81562-9f52-4cd0-d18d-b8d8ec92f561"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HouB5dVjY2IM"
      },
      "source": [
        "from __future__ import division\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "#readingsourcefile\n",
        "ToBeLabeled=0\n",
        "filename = \"/content/HelloFlooding\"\n",
        "df = pd.read_csv(filename, sep =',' , header = None, error_bad_lines = False)\n",
        "# removing first row of data then sorting according to time field\n",
        "array = df.to_numpy()\n",
        "arr = np.delete(array, 0, axis=0)\n",
        "#removing protocol column\n",
        "arr = np.delete(arr, 4, axis=1)\n",
        "array = sorted(arr, key=lambda x: float(x[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHfO7sZ6Z3bz"
      },
      "source": [
        "# storing each packet's transmission duration\n",
        "packetDurations = []\n",
        "# naming values of info field\n",
        "infoList = ['RPL Control (Destination Advertisement Object)',\n",
        "'RPL Control (DODAG Information Solicitation)',\n",
        "'RPL Control (DODAG Information Object)', 'Ack',\n",
        "'PDUType: 108 t Unknown',\n",
        "'PDUType: 112 t Unknown','PDUType: 108 \\\\t Unknown',\n",
        "'PDUType: 112 \\\\t Unknown',\n",
        "'3000 > 3001 Len=11']\n",
        "infoDict = {}\n",
        "index = 1\n",
        "for info in infoList:\n",
        "  infoDict[info] = index\n",
        "  index += 1\n",
        "#print(infoDict)\n",
        "# storing malicious nodes\n",
        "labelNodes = []\n",
        "# rewriting source, destination and info columns #\n",
        "counter = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcMw1h0uaNj5"
      },
      "source": [
        "counter = 0\n",
        "while(counter < len(array)):\n",
        "  src = array[counter][2]\n",
        "  dst = array[counter][3]\n",
        "  array[counter][2] = int(src.split(\":\").pop(), 16) if not pd.isnull(src) else src\n",
        "  if(dst == 'ff02::1a'):\n",
        "    array[counter][3] = int(9999) if not pd.isnull(dst) else dst\n",
        "  else:\n",
        "    array[counter][3] = int(dst.split(\":\").pop(), 16) if not pd.isnull(dst) else dst\n",
        "  if array[counter][5] in infoDict:\n",
        "    array[counter][5] = infoDict[array[counter][5]]\n",
        "  else:\n",
        "    array[counter][5]=None\n",
        "  if counter != 0 and counter + 1 < len(array):\n",
        "    duration = float(array[counter][1]) - float(array[counter - 1][1])\n",
        "    packetDurations.append([math.floor(float(array[counter][1])),array[counter][2],array[counter][3], duration])\n",
        "  counter += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5TllZasd-Ft",
        "outputId": "85d5ea5f-8d7d-456a-f1a5-bd069bd486ad"
      },
      "source": [
        "# Calculating node counts in per seconds #\n",
        "print(len(array))\n",
        "currentSecond = 0.0\n",
        "lastSecond = math.floor(float(array[-1][1]))\n",
        "packetsInEachSecond = dict()\n",
        "while currentSecond <= lastSecond:\n",
        "  currentSecondArray = []\n",
        "  currentRow = 1\n",
        "  while currentRow < len(array):\n",
        "    currentPacketSecond = math.floor(float(array[currentRow][1]))\n",
        "    if currentPacketSecond == currentSecond:\n",
        "      currentSecondArray.append(array[currentRow])\n",
        "    currentRow += 1\n",
        "  packetsInEachSecond[currentSecond] = currentSecondArray\n",
        "  currentSecond += 1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "143848\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vItbdymKhM8Q"
      },
      "source": [
        "# Calculating control packet counts in per seconds #\n",
        "sourceCountsBySeconds = dict()\n",
        "destinationCountsBySeconds =dict()\n",
        "daoCountsBySeconds =dict()\n",
        "disCountsBySeconds =dict()\n",
        "dioCountsBySeconds =dict()\n",
        "totalDaoDisDioBySecond =dict()\n",
        "for sec in packetsInEachSecond:\n",
        "  sourceNodeCounts = dict()\n",
        "  destinationNodeCounts =dict()\n",
        "  daoCountsByNode =dict()\n",
        "  disCountsByNode =dict()\n",
        "  dioCountsByNode =dict()\n",
        "  for packet in packetsInEachSecond[sec]:\n",
        "    src = packet[2]\n",
        "    dst = packet[3]\n",
        "    info = packet[5]\n",
        "    sourceNodeCounts[src] = 1 if src not in sourceNodeCounts else sourceNodeCounts[src] + 1\n",
        "    destinationNodeCounts[dst] = 1 if dst not in destinationNodeCounts else destinationNodeCounts[dst] + 1\n",
        "    if info == 1:\n",
        "      daoCountsByNode[src] = 1 if src not in daoCountsByNode else daoCountsByNode[src] + 1\n",
        "    elif info == 2:\n",
        "      disCountsByNode[src] = 1 if src not in disCountsByNode else disCountsByNode[src] + 1\n",
        "    elif info == 3:\n",
        "      dioCountsByNode[src] = 1 if src not in dioCountsByNode else dioCountsByNode[src] + 1\n",
        "    if info == 1 or 2 or 3:\n",
        "      totalDaoDisDioBySecond[sec] = 1 if sec not in totalDaoDisDioBySecond else totalDaoDisDioBySecond[sec] + 1\n",
        "  sourceCountsBySeconds[sec] = sourceNodeCounts\n",
        "  destinationCountsBySeconds[sec] = destinationNodeCounts\n",
        "  daoCountsBySeconds[sec] = daoCountsByNode\n",
        "  disCountsBySeconds[sec] = disCountsByNode\n",
        "  dioCountsBySeconds[sec] = dioCountsByNode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGJ6XhktxppS"
      },
      "source": [
        "# Calculating total duration times by seconds #\n",
        "transTotalDurBySec =dict()\n",
        "for sec in sourceCountsBySeconds:\n",
        "  transTotalDurBySec[sec] =dict()\n",
        "  for node in sourceCountsBySeconds[sec]:\n",
        "    transTotalDurBySec[sec][node] = 0\n",
        "rcvTotalDurBySec =dict()\n",
        "for sec in destinationCountsBySeconds:\n",
        "  rcvTotalDurBySec[sec] =dict()\n",
        "  for node in destinationCountsBySeconds[sec]:\n",
        "    rcvTotalDurBySec[sec][node] = 0\n",
        "counter = 1\n",
        "while counter < len(packetDurations):\n",
        "  second = packetDurations[counter][0]\n",
        "  try:\n",
        "    nodeTr = packetDurations[counter][1]\n",
        "    nodeRcv = packetDurations[counter][2]\n",
        "    transTotalDurBySec[second][nodeTr] = transTotalDurBySec[second][nodeTr] + packetDurations[counter][3]\n",
        "    rcvTotalDurBySec[second][nodeRcv] = rcvTotalDurBySec[second][nodeRcv] + packetDurations[counter][3]\n",
        "  except:\n",
        "    print(\"This is an error message!\")\n",
        "  counter += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khxu04Pqy3GS"
      },
      "source": [
        "# Creating enriched IoT dataset file #\n",
        "monitorArray = []\n",
        "counter = 0\n",
        "labelCounter = 0\n",
        "while counter < len(array):\n",
        "  if not pd.isnull(array[counter][2]):\n",
        "    src = array[counter][2]\n",
        "    dst = array[counter][3]\n",
        "    row = array[counter]\n",
        "    second = math.floor(float(array[counter][1]))\n",
        "    srcCount = 0.0\n",
        "    dstCount = 0.0\n",
        "    if second in sourceCountsBySeconds:\n",
        "      srcCounts = sourceCountsBySeconds[second]\n",
        "      if src in srcCounts:\n",
        "        srcCount = srcCounts[src]\n",
        "    if second in destinationCountsBySeconds:\n",
        "      dstCounts = destinationCountsBySeconds[second]\n",
        "      if dst in dstCounts:\n",
        "        dstCount = dstCounts[dst]\n",
        "    transmissionRate = srcCount / 1000.0\n",
        "    receptionRate = dstCount / 1000.0\n",
        "    TrRr = transmissionRate / receptionRate if receptionRate != 0 else 0\n",
        "    trnTtlDur = transTotalDurBySec[second][src]\n",
        "    rcvTtlDur = rcvTotalDurBySec[second][dst]\n",
        "    trnAverageTime = trnTtlDur / srcCount\n",
        "    rcvAverageTime = rcvTtlDur / dstCount\n",
        "    daoSrcCount = 0\n",
        "    disSrcCount = 0\n",
        "    dioSrcCount = 0\n",
        "    if len(daoCountsBySeconds[second]) > 0 and src in daoCountsBySeconds[second]:\n",
        "      daoSrcCount = daoCountsBySeconds[second][src]\n",
        "    else:\n",
        "      daoSrcCount = 0\n",
        "    if len(disCountsBySeconds[second]) > 0 and src in disCountsBySeconds[second]:\n",
        "      disSrcCount = disCountsBySeconds[second][src]\n",
        "    else:\n",
        "      disSrcCount = 0\n",
        "    if len(dioCountsBySeconds[second]) > 0 and src in dioCountsBySeconds[second]:\n",
        "      dioSrcCount = dioCountsBySeconds[second][src]\n",
        "    else:\n",
        "      dioSrcCount = 0\n",
        "    dao = daoSrcCount\n",
        "    dis = disSrcCount\n",
        "    dio = dioSrcCount\n",
        "    label = ToBeLabeled\n",
        "    row = np.append(row, str(transmissionRate))\n",
        "    row = np.append(row, str(receptionRate))\n",
        "    row = np.append(row, str(TrRr))\n",
        "    row = np.append(row, str(srcCount))\n",
        "    row = np.append(row, str(dstCount))\n",
        "    row = np.append(row, str(trnTtlDur))\n",
        "    row = np.append(row, str(rcvTtlDur))\n",
        "    row = np.append(row, str(trnAverageTime))\n",
        "    row = np.append(row, str(rcvAverageTime))\n",
        "    row = np.append(row, str(dao))\n",
        "    row = np.append(row, str(dis))\n",
        "    row = np.append(row, str(dio))\n",
        "    row = np.append(row, str(label))\n",
        "    monitorArray.append(row)\n",
        "  counter += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pKV9Y5Uz9b-"
      },
      "source": [
        "headers = ['No.', 'Time', 'Source', 'Destination', 'Length', 'Info',\n",
        "'Transmission Rate (per 1000 ms)',\n",
        "'Reception Rate (per 1000 ms)',\n",
        "'TR / RR',\n",
        "'Sources Count Per Sec',\n",
        "'Destinations Count Per Sec',\n",
        "'Trans Total Duration Per Sec',\n",
        "'Rcv Total Duration Per Sec',\n",
        "'Trans Average Per Sec',\n",
        "'Rcv Average Per Sec',\n",
        "'DAO',\n",
        "'DIS',\n",
        "'DIO',\n",
        "'Label'\n",
        "]\n",
        "monitorArray.insert(0, headers)\n",
        "import csv\n",
        "with open('result.csv', 'w') as monitoring:\n",
        "  wr = csv.writer(monitoring, dialect='excel', delimiter=',')\n",
        "  wr.writerows(monitorArray)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR4pV5Vq01qi",
        "outputId": "52c1e09d-e25b-4439-9561-1a8b2425f104"
      },
      "source": [
        "##########################################\n",
        "##########################################\n",
        "# Script 2 - Data Normalization Algorithm #\n",
        "##########################################\n",
        "# import the required packages\n",
        "#import pandas as pd\n",
        "#...\n",
        "# dataset headers\n",
        "#path1='./csvs/'\n",
        "headers_val = [\n",
        "'Length', 'Info',\n",
        "'Transmission Rate (per 1000 ms)',\n",
        "'Reception Rate (per 1000 ms)',\n",
        "'TR / RR',\n",
        "'Sources Count Per Sec',\n",
        "'Destinations Count Per Sec',\n",
        "'Trans Total Duration Per Sec',\n",
        "'Rcv Total Duration Per Sec',\n",
        "'Trans Average Per Sec',\n",
        "'Rcv Average Per Sec',\n",
        "'DAO',\n",
        "'DIS',\n",
        "'DIO'\n",
        "]\n",
        "amgPd = pd.DataFrame()\n",
        "# import dataset\n",
        "for chunk in pd.read_csv('result.csv', chunksize = 250000, low_memory=False):\n",
        "  amgPd = pd.concat([amgPd,chunk])\n",
        "print ('result')\n",
        "print (amgPd.describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "result\n",
            "                 No.           Time  ...            DIO     Label\n",
            "count  142686.000000  142686.000000  ...  142686.000000  142686.0\n",
            "mean    71885.777497     358.577777  ...       9.627525       0.0\n",
            "std     41534.630953     226.414291  ...      30.950901       0.0\n",
            "min         1.000000       0.000000  ...       0.000000       0.0\n",
            "25%     35903.250000     157.618106  ...       0.000000       0.0\n",
            "50%     71887.500000     349.734174  ...       0.000000       0.0\n",
            "75%    107869.750000     547.934047  ...       0.000000       0.0\n",
            "max    143848.000000     786.646335  ...     215.000000       0.0\n",
            "\n",
            "[8 rows x 19 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL-FAp0y15wq",
        "outputId": "93b068c4-3bb5-45a7-c698-69cbfe5e980b"
      },
      "source": [
        "# transform matrix format\n",
        "A = amgPd.to_numpy()\n",
        "row_num = A.shape[1]\n",
        "X = amgPd[headers_val].to_numpy()[:, 0:row_num - 1]\n",
        "Y = A[:, row_num - 1].astype(int)\n",
        "# feature normalization #\n",
        "import sklearn.preprocessing as preprocessing\n",
        "X = preprocessing.quantile_transform (X, output_distribution='normal')\n",
        "X = preprocessing.minmax_scale (X)\n",
        "data = pd.DataFrame(X)\n",
        "labels = pd.DataFrame(Y)\n",
        "#X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:2670: FutureWarning: The default value of `copy` will change from False to True in 0.23 in order to make it more consistent with the default `copy` values of other functions in :mod:`sklearn.preprocessing` and prevent unexpected side effects by modifying the value of `X` inplace. To avoid inplace modifications of `X`, it is recommended to explicitly set `copy=True`\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "xKRxo2gw1755",
        "outputId": "1d90da0e-2d6a-427e-c109-e82271d1adf9"
      },
      "source": [
        "# creating main dataset #\n",
        "amgPd = data.join(labels.rename(columns={0:'Label'}))\n",
        "print ('result after scaling')\n",
        "print (amgPd.describe())\n",
        "# transform to csv of normalized dataset\n",
        "amgPd.to_csv('0.csv', index=False)\n",
        "# import another dataset\n",
        "'''\n",
        "amgPd1 = pd.DataFrame()\n",
        "for chunk in pd.read_csv('Blackhole_Processed.csv', chunksize = 250000, low_memory=False):\n",
        "  amgPd1 = pd.concat([amgPd1,chunk])\n",
        "amgPd1.drop([u'No.','Time', u'Source', u'Destination'], axis=1, inplace=True)\n",
        "print ('BH100%10')\n",
        "print (amgPd1.describe())\n",
        "# transform matrix format\n",
        "A = amgPd1.as_matrix()\n",
        "row_num = A.shape[1]\n",
        "X = A[:, 0:row_num - 1]\n",
        "Y = A[:, row_num - 1].astype(int)\n",
        "# feature normalization #\n",
        "X = preprocessing.quantile_transform (X, output_distribution='normal')\n",
        "X = preprocessing.minmax_scale (X)\n",
        "data = pd.DataFrame(X, columns=headers_val)\n",
        "labels = pd.DataFrame(Y)\n",
        "# concatenating the datasets #\n",
        "amgPd1 = data.join(labels.rename(columns={0:'Label'}))\n",
        "print ('BH100%10 after scaling')\n",
        "print (amgPd1.describe())\n",
        "amgPd1.to_csv('BH100%10mix_norm.csv', index=False)\n",
        "# # appending to main dataset\n",
        "amgPd = pd.concat ([amgPd, amgPd1])\n",
        "amgPd.to_csv('Final.csv',index=False)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "result after scaling\n",
            "                   0              1  ...             13     Label\n",
            "count  142686.000000  139061.000000  ...  142686.000000  142686.0\n",
            "mean        0.516122       0.452393  ...       0.111241       0.0\n",
            "std         0.136887       0.227502  ...       0.244257       0.0\n",
            "min         0.000000       0.000000  ...       0.000000       0.0\n",
            "25%         0.438949       0.426137  ...       0.000000       0.0\n",
            "50%         0.530801       0.528397  ...       0.000000       0.0\n",
            "75%         0.530801       0.528397  ...       0.000000       0.0\n",
            "max         1.000000       1.000000  ...       1.000000       0.0\n",
            "\n",
            "[8 rows x 15 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\namgPd1 = pd.DataFrame()\\nfor chunk in pd.read_csv('Blackhole_Processed.csv', chunksize = 250000, low_memory=False):\\n  amgPd1 = pd.concat([amgPd1,chunk])\\namgPd1.drop([u'No.','Time', u'Source', u'Destination'], axis=1, inplace=True)\\nprint ('BH100%10')\\nprint (amgPd1.describe())\\n# transform matrix format\\nA = amgPd1.as_matrix()\\nrow_num = A.shape[1]\\nX = A[:, 0:row_num - 1]\\nY = A[:, row_num - 1].astype(int)\\n# feature normalization #\\nX = preprocessing.quantile_transform (X, output_distribution='normal')\\nX = preprocessing.minmax_scale (X)\\ndata = pd.DataFrame(X, columns=headers_val)\\nlabels = pd.DataFrame(Y)\\n# concatenating the datasets #\\namgPd1 = data.join(labels.rename(columns={0:'Label'}))\\nprint ('BH100%10 after scaling')\\nprint (amgPd1.describe())\\namgPd1.to_csv('BH100%10mix_norm.csv', index=False)\\n# # appending to main dataset\\namgPd = pd.concat ([amgPd, amgPd1])\\namgPd.to_csv('Final.csv',index=False)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54cT_FyALPA0"
      },
      "source": [
        "#Write a code to put the header columns too (later)\n",
        "final= pd.concat([pd.read_csv('0.csv'),pd.read_csv('1.csv')])\n",
        "final.to_csv('Final.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLXhXXld0iMF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}